<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: JSON | Will Anderson]]></title>
  <link href="http://willi.am/blog/categories/json/atom.xml" rel="self"/>
  <link href="http://willi.am/"/>
  <updated>2018-03-29T22:17:35-07:00</updated>
  <id>http://willi.am/</id>
  <author>
    <name><![CDATA[Will Anderson]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Consuming and Searching CSV Logs With JSON Tools]]></title>
    <link href="http://willi.am/blog/2017/09/27/consuming-and-searching-csv-logs-with-json-tools/"/>
    <updated>2017-09-27T21:47:27-07:00</updated>
    <id>http://willi.am/blog/2017/09/27/consuming-and-searching-csv-logs-with-json-tools</id>
    <content type="html"><![CDATA[<p>If you ever find yourself working with logs or traces stored as CSV files,
it&rsquo;s fairly trivial to convert them to JSON to simplify searching and manipulating them.
This blog post describes some tools that can help you filter and format the logs into something more useful.</p>

<p>The two tools we&rsquo;ll be using are <code>csv2json</code> and <code>json</code>.
These are Node modules, so you&rsquo;ll need to install Node first,
then run the following command to install the two packages globally.</p>

<p><code>
npm install -g csv2json json
</code></p>

<p>These tools should now be on your path.
To confirm, try running <code>csv2json --help</code></p>

<p>You can use the <code>csv2json</code> to convert the CSV log files into JSON files.
The JSON format places the column headers next to the column values,
which is less space efficient, but makes it much easier to read the file.</p>

<p><code>
csv2json trace.csv trace.json
</code></p>

<p>The JSON traces will then look something like this:</p>

<p><code>
[
{"PreciseTimeStamp":"3/24/2017 12:25:17 PM","ComponentGuid":"bd3facb0-1775-590d-e3ab-59f56bb23206","EventName":"DownloadStarted","AppId":"cd56de80-67c6-414a-a0af-f6b22a47b6d7"},
{"PreciseTimeStamp":"3/24/2017 12:35:17 PM","ComponentGuid":"bd3facb0-1775-590d-e3ab-59f56bb23206","EventName":"DownloadCompleted","AppId":"cd56de80-67c6-414a-a0af-f6b22a47b6d7"}
...
...
...
]
</code></p>

<p>Now at least the column names/values are closer together, so it&rsquo;s easier to see what column type you&rsquo;re dealing with.
However, it&rsquo;s still not easy to visually parse the data.
That&rsquo;s where the <code>json</code> tool comes in.
In its most basic form, it pretty-prints json, making it easier to visually parse:</p>

<p>```
cat trace.json | json
[
  {</p>

<pre><code>"PreciseTimeStamp": "3/24/2017 12:25:17 PM",
"ComponentGuid": "bd3facb0-1775-590d-e3ab-59f56bb23206",
"EventName": "DownloadStarted",
"AppId": "cd56de80-67c6-414a-a0af-f6b22a47b6d7"
</code></pre>

<p>  },
  {</p>

<pre><code>"PreciseTimeStamp": "3/24/2017 12:35:17 PM",
"ComponentGuid": "bd3facb0-175-590d-e3ab-59f56bb23206",
"EventName": "DownloadCompleted",
"AppId": "cd56de80-67c6-414a-a0af-f6b22a47b6d7"
</code></pre>

<p>  }
]
```</p>

<p>Now you can more easily scan down the columns, but it takes up a ton of space, and you probably don&rsquo;t care about most of the columns.
If you only want a few columns, you can use the <code>json -a &lt;ColumnName1&gt; &lt;ColumnName2&gt;</code> command to output specific columns in plaintext.</p>

<p><code>
type trace.json | json -a PreciseTimeStamp EventName
3/24/2017 12:25:17 PM DownloadStarted
3/24/2017 12:35:17 PM DownloadCompleted
</code></p>

<p>If you don&rsquo;t want the intermediate JSON file, you can also pipe the output of <code>csv2json</code> directly to the <code>json</code> command.
However, if you plan to run multiple calls to <code>json</code>, you will save execution time by only calling <code>csv2json</code> once.</p>

<p><code>
csv2json trace.csv | json -a PreciseTimeStamp EventName
</code></p>

<p>Also, if you already have a json file, you can provide its path to the <code>json</code> tool directly rather than using the <code>cat</code> command.
This is really just a matter of preference:</p>

<p>```
json -f trace.json -a PreciseTimeStamp EventName</p>

<h1>same as</h1>

<p>cat trace.json | json -a PreciseTimeStamp EventName
```</p>

<p>If you want to filter the logs, you can use the <code>-c</code> flag.
The syntax is JavaScript, and the <code>this</code> variable contains the JSON object for the specific row being filtered.</p>

<p>For example, to filter to a specific <code>EventName</code>, you could use something like this:</p>

<p><code>
cat trace.json | json -a PreciseTimeStamp EventName -c 'this.EventName == "DownloadCompleted"'
3/24/2017 12:35:17 PM DownloadCompleted
</code></p>

<p>There&rsquo;s lots more that the <code>json</code> tool can do. Check out the <a href="full%20docs%20here">http://trentm.com/json/</a>.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Storing Data as Newline Delimited JSON Entries]]></title>
    <link href="http://willi.am/blog/2014/07/16/storing-data-as-newline-delimited-json-entries/"/>
    <updated>2014-07-16T13:48:29-07:00</updated>
    <id>http://willi.am/blog/2014/07/16/storing-data-as-newline-delimited-json-entries</id>
    <content type="html"><![CDATA[<p>You can store data as newline delimited JSON entries.
Since JSON encodes newline characters, you can use newlines to separate entries.</p>

<p>For more details, read on.</p>

<h2>Exporting Large Data</h2>

<p>In a recent project, I needed to export data, and then import it into another system.
I decided to go with JSON as the export format, because it was structured data.
The problem was, the data could be extremely large, so loading it all into memory and generating a single JSON object wasn&rsquo;t an option.</p>

<p>As a first solution to the problem, I considered &ldquo;fake&rdquo; streaming to the file with the following pseudocode:</p>

<p>```
open the file
write &ldquo;[&rdquo;
foreach entry:</p>

<pre><code>if not first entry:
    write ","
write JSON serialized entry
</code></pre>

<p>write &ldquo;]&rdquo;
close the file
```</p>

<p>This got the job done, but what about when I import the data?
Since I wrote the data as one giant JSON array, I&rsquo;d have to load the entire thing to deserialize.
(In Node, I could use a streaming JSON parser, but this project was in PHP).</p>

<p>This means there&rsquo;s a limit to the amount of data I can import.
Even on a host where I can control PHP&rsquo;s memory limit, there&rsquo;s still a limitation to how much data can reasonably be loaded into memory.</p>

<h2>Exporting Newline Delimited Data</h2>

<p>I finally realized that I was missing something important.
JSON serializes newlines as &ldquo;\n&rdquo;, which means actual newlines can be safely used to delimit each JSON entry.
(Assuming you don&rsquo;t &ldquo;pretty print&rdquo; your JSON).</p>

<p>The new export method is as follows:</p>

<p>```
open the file
foreach entry:</p>

<pre><code>if not first entry:
    write newline
write JSON serialized entry
</code></pre>

<p>close the file
```</p>

<p>Already simpler. Yay.</p>

<h2>Importing Newline Delimited Data</h2>

<p>Now that we&rsquo;ve exported the data, it&rsquo;s time to import it.
In PHP, this is still a challenge, because there&rsquo;s no elegant solution for streams.
The closest you can get is the <a href="http://php.net/manual/en/function.stream-get-line.php"><code>stream_get_line</code></a> function.
The problem with this function is that you have to specify a max length of characters to read.
If the line is longer than those characters, you have to detect that, and keep searching until you get to the newline.
There&rsquo;s a <a href="http://php.net/manual/en/function.stream-get-line.php#109339">comment on the manual that outlines the solution</a>.</p>

<p>In pseudocode:</p>

<p>```
open the file
while get line from file:</p>

<pre><code>import entry
</code></pre>

<p>close the file
```
Again, fairly simple (ignoring caveats with &ldquo;get line from file&rdquo;).
The great thing about this solution is that we only have to load at most one entry at a time into memory.</p>

<h2>Bonus: Inspecting JSON Data From CLI</h2>

<p>If you want to inspect the data that you&rsquo;ve exported, you can use the handy &ldquo;<a href="http://npmjs.org/package/json">json</a>&rdquo; tool (requires Node).
Not only does it pretty print, but it allows you to pull out specific data, and supports newline delimited JSON entries with the <code>-g</code> flag.</p>

<p><code>
$ cat test.json
{"name":"alice"}
{"name":"bob"}
{"name":"eve"}
</code>
```
$ cat test.json | json -g
[
  {</p>

<pre><code>"name": "alice"
</code></pre>

<p>  },
  {</p>

<pre><code>"name": "bob"
</code></pre>

<p>  },
  {</p>

<pre><code>"name": "eve"
</code></pre>

<p>  }
]
<code>
</code>
$ cat test.json | json -ga name
alice
bob
eve
<code>
</code>
$ cat test.json | json -ga nameLength name -e &lsquo;this.nameLength = this.name.length&rsquo;
5 alice
3 bob
3 eve
```</p>

<p><a href="http://trentm.com/json/">Full Documentation</a></p>
]]></content>
  </entry>
  
</feed>
